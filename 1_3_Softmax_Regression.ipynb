{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_3_Softmax Regression",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "jd2EbkFAZaCz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Part 3: Softmax Regression**"
      ]
    },
    {
      "metadata": {
        "id": "DCAY3gQCZZAp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Execute this code block to install dependencies when running on colab\n",
        "try:\n",
        "    import torch\n",
        "except:\n",
        "    from os.path import exists\n",
        "    from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "    platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "    cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "    accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "    !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.0.0-{platform}-linux_x86_64.whl torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q964-azCZjI6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the second part of the lab we saw how to make a linear binary classifier using logisitic regression. In this part of the lab we'll turn our attention of multi-class classification.\n",
        "\n",
        "Softmax regression (or multinomial logistic regression) is a generalisation of logistic regression to the case where we want to handle multiple classes. In logistic regression we assumed that the labels were binary: $y_i\\in \\{0,1\\}$. We used such a classifier to distinguish between two kinds of hand-written digits. Softmax regression allows us to handle $y_i \\in \\{1,\\dots,K\\}$ where $K$ is the number of classes.\n",
        "\n",
        "Recall that in logistic regression, we had a training set $\\{(\\mathbf{x}_1,y_1),\\dots,(\\mathbf{x}_m,y_m)\\}$ of $m$ labeled examples, where the input features are $\\mathbf{x}_i \\in \\mathbb{R}^n$. In logistic regression, our hypothesis took the form:\n",
        "\n",
        "$$\\begin{align}\n",
        "h_\\theta(\\mathbf{x}) &amp;= \\frac{1}{1 + \\exp(-\\mathbf{x}^\\top\\theta)} \\equiv \\sigma(\\mathbf{x}^\\top\\theta)\n",
        "\\end{align}$$\n",
        "and the model parameters $\\theta$ were trained to minimise the cost function\n",
        "\n",
        "$$\\begin{align}\n",
        "J(\\theta) &amp; = \\sum_i y_i \\log(\\sigma(\\mathbf{x}_i^\\top\\theta)) + (1-y_i) \\log(1-\\sigma(\\mathbf{x}_i^\\top\\theta))\n",
        "\\end{align}$$\n",
        "In the softmax regression setting, we are interested in multi-class classification, and so the label $y$ can take on $K$ different values, rather than only two. Thus, in our training set $\\{(\\mathbf{x}_1,y_1),\\dots,(\\mathbf{x}_m,y_m)\\}$, we now have that $y_i \\in \\{1,\\dots,K\\}$.\n",
        "\n",
        "Given a test input $\\mathbf{x}$, we want our hypothesis to estimate the probability that $P(y=k|\\mathbf{x})$ for each value of $k=1,\\dots,K$. That is to say, we want to estimate the probability of the class label taking on each of the $K$ different possible values. Thus, our hypothesis will output a $K$-dimensional vector (whose elements sum to 1) giving us our $K$ estimated probabilities. Concretely, our hypothesis $h_\\theta(\\mathbf{x})$ takes the form:"
      ]
    },
    {
      "metadata": {
        "id": "hG5px2VoZpYN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$$\\begin{align}\n",
        "h_\\theta(\\mathbf{x}) =\n",
        "\\begin{bmatrix}\n",
        "P(y = 1 | \\mathbf{x}; \\theta) \\\\\n",
        "P(y = 2 | \\mathbf{x}; \\theta) \\\\\n",
        "\\vdots \\\\\n",
        "P(y = K | \\mathbf{x}; \\theta)\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\frac{1}{ \\sum_{j=1}^{K}{\\exp(\\theta^{(j)\\top} \\mathbf{x}) }}\n",
        "\\begin{bmatrix}\n",
        "\\exp(\\theta^{(1)\\top} \\mathbf{x} ) \\\\\n",
        "\\exp(\\theta^{(2)\\top} \\mathbf{x} ) \\\\\n",
        "\\vdots \\\\\n",
        "\\exp(\\theta^{(K)\\top} \\mathbf{x} ) \\\\\n",
        "\\end{bmatrix}\n",
        "\\end{align}$$\n",
        "Here $\\theta^{(1)},\\theta^{(2)},\\dots,\\theta^{(K)} \\in \\mathbb{R}^n$ are the parameters of our model. Notice that the term $\\frac{1}{\\sum_{j=1}^K exp(\\theta^{(j)\\top} \\mathbf{x})}$ normalizes the distribution, so that it sums to one.\n",
        "\n",
        "For convenience, we will also write $\\theta$ to denote all the parameters of our model. When you implement softmax regression, it is usually convenient to represent $\\theta$ as a $n$-by-$K$ matrix obtained by concatenating $\\theta_{(1)},\\theta^{(2)},\\dots,\\theta^{(K)}$ into columns, so that\n",
        "\n",
        "$$\\begin{align}\n",
        "\\theta = \\left[\\begin{array}{cccc}| &amp; | &amp; | &amp; | \\\\\n",
        "\\theta^{(1)} &amp; \\theta^{(2)} &amp; \\cdots &amp; \\theta^{(K)} \\\\\n",
        "| &amp; | &amp; | &amp; |\n",
        "\\end{array}\\right].\n",
        "\\end{align}$$\n",
        "Cost Function\n",
        "We now describe the cost function that we’ll use for softmax regression. In the equation below, $1\\{\\cdot\\}$ is an \"indicator function\", such that $1\\{\\mathrm{a true statement}\\}=1$, and $1\\{\\mathrm{a false statement}\\}=0$. For example, $1\\{2+2=4\\}$ evaluates to $1$; whereas $1\\{1+1=5\\}$ evaluates to $0$. Our cost function will be:\n",
        "\n",
        "$$\\begin{align}\n",
        "J(\\theta) = - \\left[ \\sum_{i=1}^{m} \\sum_{k=1}^{K}  1\\left\\{y_{i} = k\\right\\} \\log \\frac{\\exp(\\theta^{(k)\\top} \\mathbf{x}_i)}{\\sum_{j=1}^K \\exp(\\theta^{(j)\\top} \\mathbf{x}_i)}\\right]\n",
        "\\end{align}$$\n",
        "Notice that this generalises the logistic regression cost function, which could also have been written:"
      ]
    },
    {
      "metadata": {
        "id": "oFn28JjMZt0y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$$\\begin{align}\n",
        "J(\\theta) &amp;= - \\left[ \\sum_{i=1}^m   (1-y^{(i)}) \\log (1-h_\\theta(\\mathbf{x}_i)) + y^{(i)} \\log h_\\theta(\\mathbf{x}_i) \\right] \\\\\n",
        "&amp;= - \\left[ \\sum_{i=1}^{m} \\sum_{k=0}^{1} 1\\left\\{y^{(i)} = k\\right\\} \\log P(y^{(i)} = k | \\mathbf{x}_i ; \\theta) \\right]\n",
        "\\end{align}$$\n",
        "The softmax cost function is similar, except that we now sum over the $K$ different possible values of the class label. Note also that in softmax regression, we have that\n",
        "\n",
        "$$\n",
        "P(y_i = k | \\mathbf{x}_i ; \\theta) = \\frac{\\exp(\\theta^{(k)\\top} \\mathbf{x}_i)}{\\sum_{j=1}^K \\exp(\\theta^{(j)\\top} \\mathbf{x}_i) }\n",
        "$$\n",
        "We cannot solve for the minimum of $J(\\theta)$ analytically, and thus we'll resort to using gradient descent as before. Taking derivatives, one can show that the gradient is:\n",
        "\n",
        "$$\\begin{align}\n",
        "\\nabla_{\\theta^{(k)}} J(\\theta) = - \\sum_{i=1}^{m}{ \\left[ \\mathbf{x}_i \\left( 1\\{ y_i = k\\}  - P(y_i = k | \\mathbf{x}_i; \\theta) \\right) \\right]  }\n",
        "\\end{align}$$\n",
        "Armed with this formula for the derivative, one can then use it directly with a gradient descent solver (or any other 1st-order gradient based optimiser).\n",
        "\n",
        "Use the code box below to complete the implementation of the functions that return the gradients of the softmax loss function, $\\nabla_{\\theta^{(k)}} J(\\theta) \\,\\, \\forall k$ and the loss function itself, $J(\\theta)$:"
      ]
    },
    {
      "metadata": {
        "id": "6DO1k-tfPwWX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_onehot(y):\n",
        "  batch_size = y.shape[0]\n",
        "  nb_digits = max(y)+1\n",
        "  y_onehot = torch.DoubleTensor(batch_size, nb_digits)\n",
        "  y_onehot.zero_()\n",
        "  y_onehot.scatter_(1, y, 1)\n",
        "  #print(nb_digits)\n",
        "  return y_onehot\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QIUq-cSPVK_g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def softmax(X,Theta,y):\n",
        "  z= X @ Theta\n",
        "  score = torch.exp(z) # size = 100x10 \n",
        "  pdX = torch.zeros(score.shape)\n",
        "  norm_factor = torch.sum(score, dim=1) # normalization factor, size = 100x1\n",
        "  for i in range(X.shape[0]): \n",
        "    for j in range(0, max(y)+1):\n",
        "      pdX[i][j] = (score[i][j]/norm_factor[i]) \n",
        "  return pdX # returning pdf over k classes for each training example, size = 100x10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SzyzoZFAZtIL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# we wouldn't normally do this, be for this lab we want to work in double precision\n",
        "# as we'll need the numerical accuracy later on for doing checks on our gradients:\n",
        "torch.set_default_dtype(torch.float64) \n",
        "\n",
        "\n",
        "def softmax_regression_loss_grad(Theta, X, y):\n",
        "    '''Implementation of the gradient of the softmax loss function.\n",
        "    \n",
        "    Theta is the matrix of parameters, with the parameters of the k-th class in the k-th column\n",
        "    X contains the data vectors (one vector per row)\n",
        "    y is a column vector of the targets\n",
        "    '''\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    pdX = softmax(X,Theta,y)\n",
        "    y1h = make_onehot(y)\n",
        " \n",
        "    grad = -1 * (X.t() @ (y1h- pdX)) \n",
        "    #raise NotImplementedError()\n",
        "    \n",
        "    return grad\n",
        "\n",
        "def softmax_regression_loss(Theta, X, y):\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    J = torch.zeros(X.shape[0])\n",
        "    \n",
        "    pdX = softmax(X,Theta,y)\n",
        "    logprob = torch.log(pdX) \n",
        "     \n",
        "    for i in range(0, X.shape[0]):\n",
        "      for j in range(0, max(y)+1):\n",
        "        if y[i]==j:\n",
        "          idf=1\n",
        "          J[i] = idf* logprob[i][j]\n",
        "        else: \n",
        "          continue\n",
        "    loss = -1 * torch.sum(J)\n",
        "    #raise NotImplementedError()\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7GY1oAIPZ5qh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**\n",
        "Use the following code block to confirm that your implementation is correct using gradient checking. If there are problems with your gradient or loss, go back and fix them!:**"
      ]
    },
    {
      "metadata": {
        "id": "Ke6IAYCrZoGD",
        "colab_type": "code",
        "outputId": "e7b1147e-cd15-48a8-a6ce-117c869a6843",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "cell_type": "code",
      "source": [
        "# from torch.autograd import gradcheck\n",
        "from random import randrange\n",
        "\n",
        "def grad_check(f, x, analytic_grad, num_checks=10, h=1e-3):\n",
        "    sum_error = 0\n",
        "    for i in range(num_checks):\n",
        "        ix = tuple([randrange(m) for m in x.shape]) #randomly sample value to change\n",
        "\n",
        "        oldval = x[ix].item()\n",
        "        x[ix] = oldval + h # increment by h\n",
        "        fxph = f(x) # evaluate f(x + h)\n",
        "        x[ix] = oldval - h # increment by h\n",
        "        fxmh = f(x) # evaluate f(x - h)\n",
        "        x[ix] = oldval # reset\n",
        "\n",
        "        grad_numerical = (fxph - fxmh) / (2 * h)\n",
        "        grad_analytic = analytic_grad[ix]\n",
        "        rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic) + 1e-8)\n",
        "        sum_error += rel_error\n",
        "        print('numerical: %f\\tanalytic: %f\\trelative error: %e' % (grad_numerical, grad_analytic, rel_error))\n",
        "    return sum_error / num_checks\n",
        "\n",
        "# Create some test data:\n",
        "num_classes = 10\n",
        "features_dim = 20\n",
        "num_items = 100\n",
        "Theta = torch.randn((features_dim, num_classes))\n",
        "X = torch.randn((num_items,features_dim))\n",
        "y = torch.torch.randint(0, num_classes, (num_items, 1))\n",
        "\n",
        " \n",
        "# compute the analytic gradient\n",
        "grad = softmax_regression_loss_grad(Theta, X, y)\n",
        "    \n",
        "# run the gradient checker    \n",
        "grad_check(lambda th: softmax_regression_loss(th, X, y), Theta, grad)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numerical: -5.540699\tanalytic: -5.540699\trelative error: 6.612041e-09\n",
            "numerical: -3.375617\tanalytic: -3.375617\trelative error: 3.361221e-08\n",
            "numerical: -4.100808\tanalytic: -4.100808\trelative error: 3.197206e-08\n",
            "numerical: 3.599901\tanalytic: 3.599901\trelative error: 2.533664e-08\n",
            "numerical: 1.376865\tanalytic: 1.376864\trelative error: 7.448812e-08\n",
            "numerical: -1.688989\tanalytic: -1.688989\trelative error: 1.373867e-09\n",
            "numerical: -4.853264\tanalytic: -4.853263\trelative error: 1.927296e-08\n",
            "numerical: 6.131145\tanalytic: 6.131145\trelative error: 6.256986e-09\n",
            "numerical: -0.562739\tanalytic: -0.562739\trelative error: 5.593640e-09\n",
            "numerical: 2.449983\tanalytic: 2.449983\trelative error: 3.884091e-08\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.4336e-08)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "J8-Ni5bY8NY0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Theta = torch.Tensor([[1, 0], [0, 1]]) # Theta = 2x2\n",
        " \n",
        "X = torch.Tensor([[1, 0], [0, 1]]) # X = 2x2\n",
        " \n",
        "y = torch.LongTensor([[0], [1]]) # y= 2x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tDs9FT2MZiSK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "assert torch.abs(softmax_regression_loss(Theta, X, y) - 0.6265) < 0.0001\n",
        "grad = softmax_regression_loss_grad(Theta, X, y)\n",
        "assert torch.torch.allclose(torch.abs(grad/0.2689), torch.ones_like(grad), atol=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t0Fcg3VxaJ_L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Training Softmax regression with gradient descent on real data**\n",
        "\n",
        "We'll now try gradient descent with our softmax regression using the digits dataset. As before, when we looked at logistic regression, we load the data and create test and training sets. Note that this time we'll use all the classes:"
      ]
    },
    {
      "metadata": {
        "id": "RA7zIKSLaWIX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now define a simple gradient descent loop to train the model:"
      ]
    },
    {
      "metadata": {
        "id": "62WegyuoaLl9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "\n",
        "X, y = (torch.Tensor(z) for z in load_digits(10, True)) #convert to pytorch Tensors\n",
        "X = torch.cat((X, torch.ones((X.shape[0], 1))), 1) # append a column of 1's to the X's\n",
        "X /= 255\n",
        "y = y.reshape(-1, 1) # reshape y into a column vector\n",
        "y = y.type(torch.LongTensor)\n",
        "\n",
        "# We're also going to break the data into a training set for computing the regression parameters\n",
        "# and a test set to evaluate the predictive ability of those parameters\n",
        "perm = torch.randperm(y.shape[0])\n",
        "X_train = X[perm[0:260], :]\n",
        "y_train = y[perm[0:260]]\n",
        "X_test = X[perm[260:], :]\n",
        "y_test = y[perm[260:]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VxMTVs6NaU-U",
        "colab_type": "code",
        "outputId": "ec697a9e-b252-4a72-f9ce-4bf715c910cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "cell_type": "code",
      "source": [
        "alpha = 0.1\n",
        "theta_gd = torch.rand((X_train.shape[1], 10))\n",
        "\n",
        "for e in range(0, 1000):\n",
        "    gr = softmax_regression_loss_grad(theta_gd, X_train, y_train)\n",
        "    theta_gd -= alpha * gr\n",
        "    if e%100 == 0:\n",
        "        print(\"Training Loss: \", softmax_regression_loss(theta_gd, X_train, y_train))\n",
        "\n",
        "# Compute the accuracy of the test set\n",
        "proba = torch.softmax(X_test @ theta_gd, 1)\n",
        "print(float((proba.argmax(1)-y_test[:,0]==0).sum()) / float(proba.shape[0]))\n",
        "print()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Loss:  tensor(590.7208)\n",
            "Training Loss:  tensor(275.3403)\n",
            "Training Loss:  tensor(175.6439)\n",
            "Training Loss:  tensor(132.7326)\n",
            "Training Loss:  tensor(109.0063)\n",
            "Training Loss:  tensor(93.7598)\n",
            "Training Loss:  tensor(82.9978)\n",
            "Training Loss:  tensor(74.9112)\n",
            "Training Loss:  tensor(68.5612)\n",
            "Training Loss:  tensor(63.4099)\n",
            "0.936890045543266\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "27wif6ZFacz6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Running the above, you should observe that the training loss decreases over time. The final accuracy on the test set is also printed and should be around 90% (it will depend on the particular training/test splits you generated as well as the initial parameters for the softmax).\n",
        "\n",
        "## Overparameterisation in softmax regression\n",
        "Softmax regression has an unusual property that it has a \"redundant\" set of parameters. To explain what this means, suppose we take each of our parameter vectors $\\theta^{(j)}$, and subtract some fixed vector $\\psi$ from it, so that every $\\theta^{(j)}$ is now replaced with $\\theta^{(j)}−\\psi$ (for every $j=1,\\dots,k$). Our hypothesis now estimates the class label probabilities as\n",
        "\n",
        "$$\\begin{align}\n",
        "P(y^{(i)} = k | x^{(i)} ; \\theta)\n",
        "&amp;= \\frac{\\exp((\\theta^{(k)}-\\psi)^\\top x^{(i)})}{\\sum_{j=1}^K \\exp( (\\theta^{(j)}-\\psi)^\\top x^{(i)})}  \\\\\n",
        "&amp;= \\frac{\\exp(\\theta^{(k)\\top} x^{(i)}) \\exp(-\\psi^\\top x^{(i)})}{\\sum_{j=1}^K \\exp(\\theta^{(j)\\top} x^{(i)}) \\exp(-\\psi^\\top x^{(i)})} \\\\\n",
        "&amp;= \\frac{\\exp(\\theta^{(k)\\top} x^{(i)})}{\\sum_{j=1}^K \\exp(\\theta^{(j)\\top} x^{(i)})}.\n",
        "\\end{align}$$\n",
        "In other words, subtracting $\\psi$ from every $\\theta^{(j)}$ does not affect our hypothesis’ predictions at all!</strong> This shows that softmax regression’s parameters are \"redundant\". More formally, we say that our softmax model is \"overparameterised\" meaning that for any hypothesis we might fit to the data, there are multiple parameter settings that give rise to exactly the same hypothesis function $h_\\theta$ mapping from inputs $\\mathbf{x}$ to the predictions.\n",
        "\n",
        "Further, if the cost function $J(\\theta)$ is minimized by some setting of the parameters $(\\theta^{(1)},\\theta^{(2)},\\dots,\\theta^{(k)})$, then it is also minimised by $\\theta^{(1)}-\\psi,\\theta^{(2)}-\\psi,\\dots,\\theta^{(k)}-\\psi)$ for any value of $\\psi$. Thus, the minimiser of $J(\\theta)$ is not unique.\n",
        "\n",
        "(Interestingly, $J(\\theta)$ is still convex, and thus gradient descent will not run into local optima problems. The Hessian is however singular/non-invertible, which causes a straightforward implementation of Newton's method (a second-order optimiser) to run into numerical problems.)"
      ]
    },
    {
      "metadata": {
        "id": "Qm3ZLM7Papxq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Notice also that by setting $\\psi=\\theta^{(K)}$, one can always replace $\\theta^{(K)}$ with $\\theta^{(K)}-\\psi=\\mathbf{0}$ (the vector of all $0$’s), without affecting the hypothesis. Thus, one could \"eliminate\" the vector of parameters $\\theta^{(K)}$ (or any other $\\theta^{(k)}$, for any single value of $k$), without harming the representational power of our hypothesis. Indeed, rather than optimising over the $K \\cdot n$ parameters $(\\theta^{(1)},\\theta^{(2)},\\dots,\\theta^{(k)})$ (where $\\theta^{(k)} \\in \\mathbb{R}^n$, one can instead set $\\theta^{(K)}=\\mathbf{0}$ and optimize only with respect to the $(K-1) \\cdot n$ remaining parameters.\n",
        "\n",
        " **Use the following block to implement the softmax gradients for the case where the final column of the parameters theta is fixed to be zero:**"
      ]
    },
    {
      "metadata": {
        "id": "gX3rpWyYacFq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "def softmax_regression_loss_grad_0(Theta, X, y):\n",
        "    '''Implementation of the gradient of the softmax loss function, with the parameters of the\n",
        "    last class fixed to be zero.\n",
        "    \n",
        "    Theta is the matrix of parameters, with the parameters of the k-th class in the k-th column; \n",
        "            K-1 classes are included, and the parameters of the last class are implicitly zero.\n",
        "    X contains the data vectors (one vector per row)\n",
        "    y is a column vector of the targets\n",
        "    '''\n",
        "    \n",
        "    # add the missing column of zeros:\n",
        "    Theta = torch.cat((Theta, torch.zeros(Theta.shape[0],1)), 1)\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    m = X.shape[0]\n",
        "    \n",
        "    z= X @ Theta\n",
        "    z[:][-1] = 0\n",
        "    score = torch.exp(z) # size = 100x10 \n",
        "    pdX = torch.zeros(score.shape)\n",
        "    \n",
        "    norm_factor = torch.sum(score, dim=1) # normalization factor, size = 100x1\n",
        "    \n",
        "    for i in range(X.shape[0]): \n",
        "      for j in range(0, max(y)+1):\n",
        "        pdX[i][j] = (score[i][j]/norm_factor[i]).double()\n",
        "        \n",
        "    #pdX = softmax(X,Theta,y)\n",
        "    y1h = make_onehot(y)\n",
        " \n",
        "    grad = -1 * (X.t() @ (y1h- pdX)).double()\n",
        "    #raise NotImplementedError()\n",
        "    \n",
        "    # remove the last column from the gradients\n",
        "    grad = grad[0:grad.shape[0], 0:grad.shape[1]-1]\n",
        "    return grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3sLOALQFazor",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Theta = torch.Tensor([[1, 0], [0, 0]])\n",
        "X = torch.Tensor([[1, 0], [0, 1]])\n",
        "y = torch.LongTensor([[0], [1]])\n",
        "\n",
        "grad = softmax_regression_loss_grad(Theta, X, y)\n",
        "grad0 = softmax_regression_loss_grad_0(Theta[:,0:grad.shape[1]-1], X, y)\n",
        "assert torch.torch.allclose(grad[:,0:grad.shape[1]-1], grad0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w4eg6yG8a4y0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, we can run gradient descent with our reduced paramter gradient function, and confirm that the results are similar to before:"
      ]
    },
    {
      "metadata": {
        "id": "Q3dRql-ka4Cc",
        "colab_type": "code",
        "outputId": "54f2e8f9-6ee1-4945-c905-7a4ff54b80bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "alpha = 0.1\n",
        "theta_gd = torch.rand((X_train.shape[1], 9))\n",
        "\n",
        "for e in range(0, 1000):\n",
        "    gr = softmax_regression_loss_grad_0(theta_gd, X_train, y_train)\n",
        "    theta_gd -= alpha * gr\n",
        "\n",
        "theta_gd = torch.cat((theta_gd, torch.zeros(theta_gd.shape[0], 1)), 1)\n",
        "proba = torch.softmax(X_test @ theta_gd, 1)\n",
        "print(float((proba.argmax(1)-y_test[:,0]==0).sum()) / float(proba.shape[0]))\n",
        "print()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9225764476252439\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}